# -*- coding: utf-8 -*-
"""Titanic_Survival.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AlOhrIcbeuMzK8VcA8IK203hR7nVPXsE
"""

# Data handling
import pandas as pd
import numpy as np

# Visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Machine learning (will use later)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from google.colab import files
uploaded = files.upload()

import io

# Load training data
train_df = pd.read_csv(io.BytesIO(uploaded['train.csv']))
test_df = pd.read_csv(io.BytesIO(uploaded['test.csv']))

# Show the first 5 rows
train_df.head()

# See basic info about columns, data types, and null values
train_df.info()

# Count missing values in each column
train_df.isnull().sum()

# Basic statistics for numerical columns
train_df.describe()

train_df.head()

sns.countplot(x='Survived', data=train_df)
plt.title('Survival Count (0 = Died, 1 = Survived)')
plt.show()

sns.countplot(x='Survived', hue='Sex', data=train_df)
plt.title('Survival Count by Gender')
plt.show()

sns.countplot(x='Survived', hue='Pclass', data=train_df)
plt.title('Survival by Passenger Class')
plt.show()

plt.figure(figsize=(8, 4))
sns.histplot(train_df['Age'].dropna(), bins=30, kde=True)
plt.title('Age Distribution')
plt.show()

# Only use numeric columns for correlation
numeric_df = train_df.select_dtypes(include=['int64', 'float64'])

plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

train_df.isnull().sum()

train_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())
train_df['Embarked'] = train_df['Embarked'].fillna(train_df['Embarked'].mode()[0])

if 'Cabin' in train_df.columns:
    train_df.drop('Cabin', axis=1, inplace=True)

# Convert Sex column to numeric
train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})

# One-hot encode Embarked column
embarked_dummies = pd.get_dummies(train_df['Embarked'], prefix='Embarked')
train_df = pd.concat([train_df, embarked_dummies], axis=1)
train_df.drop('Embarked', axis=1, inplace=True)

train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1

train_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)

print("Missing values after cleaning:")
print(train_df.isnull().sum())

# Convert 'Sex' to numeric (assuming 'Sex' column exists)
if 'Sex' in train_df.columns:
    train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})

# One-hot encode 'Embarked' if it exists
if 'Embarked' in train_df.columns:
    embarked_dummies = pd.get_dummies(train_df['Embarked'], prefix='Embarked')
    train_df = pd.concat([train_df, embarked_dummies], axis=1)
    train_df.drop('Embarked', axis=1, inplace=True)

# Create 'FamilySize' feature if SibSp and Parch exist
if 'SibSp' in train_df.columns and 'Parch' in train_df.columns:
    train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1

# Drop unused columns if they exist
columns_to_drop = ['PassengerId', 'Name', 'Ticket']
for col in columns_to_drop:
    if col in train_df.columns:
        train_df.drop(col, axis=1, inplace=True)

# Show the first few rows of cleaned dataframe
train_df.head()

print("Any missing values in X_train?", X_train.isnull().any().any())
print("Any infinite values in X_train?", np.isinf(X_train).any().any())

print("Missing values in X_train per column:")
print(X_train.isnull().sum())

print("\nMissing values in X_test per column:")
print(X_test.isnull().sum())

# Make sure 'Sex' is mapped correctly
train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})

# One-hot encode 'Embarked'
if 'Embarked' in train_df.columns:
    embarked_dummies = pd.get_dummies(train_df['Embarked'], prefix='Embarked')
    train_df = pd.concat([train_df, embarked_dummies], axis=1)
    train_df.drop('Embarked', axis=1, inplace=True)

# Create 'FamilySize' feature
train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1

# Drop unnecessary columns
columns_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']
for col in columns_to_drop:
    if col in train_df.columns:
        train_df.drop(col, axis=1, inplace=True)

# Fill missing Age and Embarked (again just in case)
train_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())
# Embarked already handled above, but check for any missing:
if 'Embarked' in train_df.columns:
    train_df['Embarked'] = train_df['Embarked'].fillna(train_df['Embarked'].mode()[0])

X = train_df.drop('Survived', axis=1)
y = train_df['Survived']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Check again for missing values
print("Missing values in X_train after split:\n", X_train.isnull().sum())

for col in X_train.columns:
    median_val = X_train[col].median()
    X_train[col].fillna(median_val, inplace=True)
    X_test[col].fillna(median_val, inplace=True)

print("Missing values in X_train:")
print(X_train.isnull().sum())

print("\nMissing values in X_test:")
print(X_test.isnull().sum())

for col in X_train.columns:
    if X_train[col].isnull().any() or X_test[col].isnull().any():
        median_val = X_train[col].median()
        X_train[col] = X_train[col].fillna(median_val)
        X_test[col] = X_test[col].fillna(median_val)

print("After filling:")

print(X_train.isnull().sum())
print(X_test.isnull().sum())

train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})

print(train_df['Sex'].isnull().sum())

print(train_df['Sex'].unique())

print(train_df.columns.tolist())

train_df_original = pd.read_csv('train.csv')  # or reload the dataset again
print(train_df_original['Sex'].unique())
print(train_df_original.head())

train_df = pd.read_csv('train.csv')

# Fill missing Age
train_df['Age'] = train_df['Age'].fillna(train_df['Age'].median())

# Fill missing Embarked
train_df['Embarked'] = train_df['Embarked'].fillna(train_df['Embarked'].mode()[0])

# Convert Sex to numeric
train_df['Sex'] = train_df['Sex'].map({'male': 0, 'female': 1})

# One-hot encode Embarked
embarked_dummies = pd.get_dummies(train_df['Embarked'], prefix='Embarked')
train_df = pd.concat([train_df, embarked_dummies], axis=1)
train_df.drop('Embarked', axis=1, inplace=True)

# Create FamilySize feature
train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1

# Drop unnecessary columns
train_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)

print(train_df.isnull().sum())

X = train_df.drop('Survived', axis=1)
y = train_df['Survived']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

for col in X_train.columns:
    median_val = X_train[col].median()
    X_train[col] = X_train[col].fillna(median_val)
    X_test[col] = X_test[col].fillna(median_val)

X_train = X_train.astype(float)
X_test = X_test.astype(float)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=500)
model.fit(X_train, y_train)

# Train the model
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)

# Evaluate accuracy
from sklearn.metrics import accuracy_score
print("Accuracy:", accuracy_score(y_test, y_pred))